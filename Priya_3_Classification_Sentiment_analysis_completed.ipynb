{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Problem statement: Classification model to analyze Amazon product reviews\n",
        "\n",
        "The objective is to create a classification model that will analyze Amazon product reviews to classify sentiments as positive or negative. Here's a breakdown of the steps involved in this workflow:\n",
        "\n",
        "- Step 1: Load the Dataset\n",
        "- Step 2: Data Pre-processing\n",
        "- Step 3: Feature Selection\n",
        "- Step 4: Model Selection\n",
        "- Step 5: Training the Model\n",
        "- Step 6: Model Evaluation\n",
        "- Step 7: Hyperparameter Tuning\n",
        "- Step 8: Cross Validation\n",
        "\n",
        "The notebook contains 7 exercises in total:\n",
        "\n",
        "* [Exercise 1](#ex_1)\n",
        "* [Exercise 2](#ex_2)\n",
        "* [Exercise 3](#ex_3)\n",
        "* [Exercise 4](#ex_4)\n",
        "* [Exercise 5](#ex_5)\n",
        "* [Exercise 6](#ex_6)\n",
        "* [Exercise 7](#ex_7)"
      ],
      "metadata": {
        "id": "IRGQ0MBqOnEL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uH6KQFuh1XHu"
      },
      "source": [
        "## Step 1: Load the dataset\n",
        "First, let's load the dataset from Google Drive. You need to upload the dataset and then read the CSV file into a pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "MGvvWInefqU-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "d1a81565-18a9-4b01-cd6a-6673e177f0ba"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f1bf6af4-f750-49f3-9e8b-587ea0c6d87c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f1bf6af4-f750-49f3-9e8b-587ea0c6d87c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving amazon-product-review-data.csv to amazon-product-review-data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hfYy3kh11GCA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "c6d7ae4d-e76c-439e-eac4-2c50ab122b60"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  market_place customer_id         review_id    product_id product_parent  \\\n",
              "0         \"US\"  \"42521656\"  \"R26MV8D0KG6QI6\"  \"B000SAQCWC\"    \"159713740\"   \n",
              "1         \"US\"  \"12049833\"  \"R1OF8GP57AQ1A0\"  \"B00509LVIQ\"    \"138680402\"   \n",
              "2         \"US\"    \"107642\"  \"R3VDC1QB6MC4ZZ\"  \"B00KHXESLC\"    \"252021703\"   \n",
              "3         \"US\"   \"6042304\"  \"R12FA3DCF8F9ER\"  \"B000F8JIIC\"    \"752728342\"   \n",
              "4         \"US\"  \"18123821\"   \"RTWHVNV6X4CNJ\"  \"B004ZWR9RQ\"    \"552138758\"   \n",
              "\n",
              "                                       product_title product_category  \\\n",
              "0  \"The Cravings Place Chocolate Chunk Cookie Mix...        \"Grocery\"   \n",
              "1          \"Mauna Loa Macadamias, 11 Ounce Packages\"        \"Grocery\"   \n",
              "2  \"Organic Matcha Green Tea Powder - 100% Pure M...        \"Grocery\"   \n",
              "3  \"15oz Raspberry Lyons Designer Dessert Syrup S...        \"Grocery\"   \n",
              "4  \"Stride Spark Kinetic Fruit Sugar Free Gum, 14...        \"Grocery\"   \n",
              "\n",
              "   star_rating  helpful_votes  total_votes     vine verified_purchase  \\\n",
              "0            1              0            0  0 \\t(N)           1 \\t(Y)   \n",
              "1            1              0            0  0 \\t(N)           1 \\t(Y)   \n",
              "2            1              0            0  0 \\t(N)           0 \\t(N)   \n",
              "3            1              0            0  0 \\t(N)           1 \\t(Y)   \n",
              "4            1              0            0  0 \\t(N)           1 \\t(Y)   \n",
              "\n",
              "                        review_headline  \\\n",
              "0  \"Using these for years - love them.\"   \n",
              "1                           \"Wonderful\"   \n",
              "2                          \"Five Stars\"   \n",
              "3                          \"Five Stars\"   \n",
              "4                          \"Five Stars\"   \n",
              "\n",
              "                                         review_body review_date sentiments  \n",
              "0  \"As a family allergic to wheat, dairy, eggs, n...  2015-08-31   positive  \n",
              "1  \"My favorite nut. Creamy, crunchy, salty, and ...  2015-08-31   positive  \n",
              "2  \"This green tea tastes so good! My girlfriend ...  2015-08-31   positive  \n",
              "3  \"I love Melissa's brand but this is a great se...  2015-08-31   positive  \n",
              "4                                             \"good\"  2015-08-31   positive  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2c03d803-33fe-497f-82a3-d3567b01a921\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>market_place</th>\n",
              "      <th>customer_id</th>\n",
              "      <th>review_id</th>\n",
              "      <th>product_id</th>\n",
              "      <th>product_parent</th>\n",
              "      <th>product_title</th>\n",
              "      <th>product_category</th>\n",
              "      <th>star_rating</th>\n",
              "      <th>helpful_votes</th>\n",
              "      <th>total_votes</th>\n",
              "      <th>vine</th>\n",
              "      <th>verified_purchase</th>\n",
              "      <th>review_headline</th>\n",
              "      <th>review_body</th>\n",
              "      <th>review_date</th>\n",
              "      <th>sentiments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"US\"</td>\n",
              "      <td>\"42521656\"</td>\n",
              "      <td>\"R26MV8D0KG6QI6\"</td>\n",
              "      <td>\"B000SAQCWC\"</td>\n",
              "      <td>\"159713740\"</td>\n",
              "      <td>\"The Cravings Place Chocolate Chunk Cookie Mix...</td>\n",
              "      <td>\"Grocery\"</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0 \\t(N)</td>\n",
              "      <td>1 \\t(Y)</td>\n",
              "      <td>\"Using these for years - love them.\"</td>\n",
              "      <td>\"As a family allergic to wheat, dairy, eggs, n...</td>\n",
              "      <td>2015-08-31</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"US\"</td>\n",
              "      <td>\"12049833\"</td>\n",
              "      <td>\"R1OF8GP57AQ1A0\"</td>\n",
              "      <td>\"B00509LVIQ\"</td>\n",
              "      <td>\"138680402\"</td>\n",
              "      <td>\"Mauna Loa Macadamias, 11 Ounce Packages\"</td>\n",
              "      <td>\"Grocery\"</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0 \\t(N)</td>\n",
              "      <td>1 \\t(Y)</td>\n",
              "      <td>\"Wonderful\"</td>\n",
              "      <td>\"My favorite nut. Creamy, crunchy, salty, and ...</td>\n",
              "      <td>2015-08-31</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"US\"</td>\n",
              "      <td>\"107642\"</td>\n",
              "      <td>\"R3VDC1QB6MC4ZZ\"</td>\n",
              "      <td>\"B00KHXESLC\"</td>\n",
              "      <td>\"252021703\"</td>\n",
              "      <td>\"Organic Matcha Green Tea Powder - 100% Pure M...</td>\n",
              "      <td>\"Grocery\"</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0 \\t(N)</td>\n",
              "      <td>0 \\t(N)</td>\n",
              "      <td>\"Five Stars\"</td>\n",
              "      <td>\"This green tea tastes so good! My girlfriend ...</td>\n",
              "      <td>2015-08-31</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"US\"</td>\n",
              "      <td>\"6042304\"</td>\n",
              "      <td>\"R12FA3DCF8F9ER\"</td>\n",
              "      <td>\"B000F8JIIC\"</td>\n",
              "      <td>\"752728342\"</td>\n",
              "      <td>\"15oz Raspberry Lyons Designer Dessert Syrup S...</td>\n",
              "      <td>\"Grocery\"</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0 \\t(N)</td>\n",
              "      <td>1 \\t(Y)</td>\n",
              "      <td>\"Five Stars\"</td>\n",
              "      <td>\"I love Melissa's brand but this is a great se...</td>\n",
              "      <td>2015-08-31</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"US\"</td>\n",
              "      <td>\"18123821\"</td>\n",
              "      <td>\"RTWHVNV6X4CNJ\"</td>\n",
              "      <td>\"B004ZWR9RQ\"</td>\n",
              "      <td>\"552138758\"</td>\n",
              "      <td>\"Stride Spark Kinetic Fruit Sugar Free Gum, 14...</td>\n",
              "      <td>\"Grocery\"</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0 \\t(N)</td>\n",
              "      <td>1 \\t(Y)</td>\n",
              "      <td>\"Five Stars\"</td>\n",
              "      <td>\"good\"</td>\n",
              "      <td>2015-08-31</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2c03d803-33fe-497f-82a3-d3567b01a921')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2c03d803-33fe-497f-82a3-d3567b01a921 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2c03d803-33fe-497f-82a3-d3567b01a921');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b5068992-bc69-4832-b583-1d83ac40d9f6\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b5068992-bc69-4832-b583-1d83ac40d9f6')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b5068992-bc69-4832-b583-1d83ac40d9f6 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 500,\n  \"fields\": [\n    {\n      \"column\": \"market_place\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\\\"US\\\"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"customer_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 491,\n        \"samples\": [\n          \"\\\"42114621\\\"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"review_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 500,\n        \"samples\": [\n          \"\\\"R1II85ETIHO6VK\\\"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"product_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 496,\n        \"samples\": [\n          \"\\\"0062510630\\\"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"product_parent\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 495,\n        \"samples\": [\n          \"\\\"735635617\\\"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"product_title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 496,\n        \"samples\": [\n          \"\\\"Earth Medicine: Ancestor's Ways of Harmony for Many Moons\\\"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"product_category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"\\\"Digital_Ebook_Purchase\\\"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"star_rating\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"helpful_votes\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 36,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          12\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_votes\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 0,\n        \"max\": 47,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"vine\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"1 \\t(Y)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"verified_purchase\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"0 \\t(N)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"review_headline\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 337,\n        \"samples\": [\n          \"\\\"It is a great substitute for regular coffee and I highly recommend it\\\"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"review_body\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 494,\n        \"samples\": [\n          \"\\\"The book covers some of the general things that happen in Utah but i was disappointed in the lack of specifics and the post modern view this woman has.\\\"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"review_date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"2013-09-09\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiments\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"negative\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a DataFrame\n",
        "df = pd.read_csv('amazon-product-review-data.csv')\n",
        "\n",
        "# Display the first few rows to check if the data is loaded correctly\n",
        "df.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_values = df['sentiments'].unique()\n",
        "print(unique_values)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WeixsnxUqYw",
        "outputId": "79c0ef97-6e77-48e2-d529-31510e7dd5d3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['positive' 'negative']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "value_counts = df['sentiments'].value_counts()\n",
        "print(value_counts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcCG8RiAVEJP",
        "outputId": "5e0e151f-df75-4ba0-cc6c-a3279c9a8314"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive    398\n",
            "negative    102\n",
            "Name: sentiments, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8E6a6h_2LIC"
      },
      "source": [
        "## Step 2: Data Pre-processing\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "t_WMCjp72Z3g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b360a4d2-0399-43b4-9920-3054f60a23d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "**************\n",
            "[1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1\n",
            " 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
            " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1\n",
            " 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 0 1 1 1\n",
            " 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0\n",
            " 1 1 1 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1\n",
            " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1\n",
            " 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1\n",
            " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 0 1\n",
            " 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1\n",
            " 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1]\n",
            "X_train shape: (400, 3466)\n",
            "X_test shape: (100, 3466)\n",
            "y_train shape: (400,)\n",
            "y_test shape: (100,)\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries for data pre-processing\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Remove any rows with missing values\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Encode the 'sentiments' column (positive/negative) to numerical values (0/1)\n",
        "le = LabelEncoder()\n",
        "df['sentiments'] = le.fit_transform(df['sentiments'])\n",
        "\n",
        "# Text data preprocessing using TF-IDF vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
        "X = tfidf_vectorizer.fit_transform(df['review_body']).toarray()\n",
        "y = df['sentiments'].values\n",
        "\n",
        "print(X)\n",
        "print('**************')\n",
        "print(y)\n",
        "# # Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Display the shapes of the resulting data\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "# Apply feature selection using chi-squared (chi2) test\n",
        "# You can adjust the number of features (k) as needed\n",
        "k = 1000\n",
        "selector = SelectKBest(chi2, k=k)\n",
        "X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "X_test_selected = selector.transform(X_test)\n",
        "\n",
        "# Display the shapes of the selected feature sets\n",
        "print(\"X_train_selected shape:\", X_train_selected.shape)\n",
        "print(\"X_test_selected shape:\", X_test_selected.shape)\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(random_state=42, class_weight='balanced')\n",
        "# Train the Logistic Regression model on the selected features\n",
        "model.fit(X_train_selected, y_train)\n",
        "# Predict sentiment labels for the test data\n",
        "y_pred = model.predict(X_test_selected)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Display a classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Display a confusion matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DL4QsK6EWf4V",
        "outputId": "c9cf3b58-6cb2-4f40-9edd-3fb7b24279b8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train_selected shape: (400, 1000)\n",
            "X_test_selected shape: (100, 1000)\n",
            "Accuracy: 0.82\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.30      0.21      0.25        14\n",
            "           1       0.88      0.92      0.90        86\n",
            "\n",
            "    accuracy                           0.82       100\n",
            "   macro avg       0.59      0.57      0.57       100\n",
            "weighted avg       0.80      0.82      0.81       100\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 3 11]\n",
            " [ 7 79]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"ex_1\"></a>\n",
        "## Exercise 1\n",
        "\n",
        "- Use the train_test_split function and change the test_size to 0.3\n",
        "\n",
        "This way the training set (X and y) should be 70% and the testing set(X and y) should be 30%"
      ],
      "metadata": {
        "id": "eSAPAkPnTXrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Write your code here\n",
        "# Import necessary libraries for data pre-processing\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Remove any rows with missing values\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Encode the 'sentiments' column (positive/negative) to numerical values (0/1)\n",
        "le = LabelEncoder()\n",
        "df['sentiments'] = le.fit_transform(df['sentiments'])\n",
        "\n",
        "# Text data preprocessing using TF-IDF vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
        "X = tfidf_vectorizer.fit_transform(df['review_body']).toarray()\n",
        "y = df['sentiments'].values\n",
        "\n",
        "# Split the data into training and testing sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Display the shapes of the resulting data\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n"
      ],
      "metadata": {
        "id": "-UWf6agSRon9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "271f7c5e-04bc-4c02-c6eb-f6848f2db689"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (350, 3466)\n",
            "X_test shape: (150, 3466)\n",
            "y_train shape: (350,)\n",
            "y_test shape: (150,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9q_p7vr27mV"
      },
      "source": [
        "## Step 3: Feature Selection\n",
        "\n",
        "In this step, we'll perform feature selection to reduce the dimensionality of the TF-IDF vectorized data and potentially improve the model's performance. We'll use feature selection techniques like chi-squared (chi2) or mutual information to select the most important features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8usH1IZP2-HS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91f766b1-6055-4cfb-a0f7-bd889c5e99bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train_selected shape: (350, 1000)\n",
            "X_test_selected shape: (150, 1000)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "# Apply feature selection using chi-squared (chi2) test\n",
        "# You can adjust the number of features (k) as needed\n",
        "k = 1000\n",
        "selector = SelectKBest(chi2, k=k)\n",
        "X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "X_test_selected = selector.transform(X_test)\n",
        "\n",
        "# Display the shapes of the selected feature sets\n",
        "print(\"X_train_selected shape:\", X_train_selected.shape)\n",
        "print(\"X_test_selected shape:\", X_test_selected.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"ex_2\"></a>\n",
        "## Exercise 2\n",
        "\n",
        "- Compare the X_train_selected shape and X_test_selected shape with the new test_size=0.3"
      ],
      "metadata": {
        "id": "SU3KOde8Te0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Write your code here\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "# Apply feature selection using chi-squared (chi2) test\n",
        "# You can adjust the number of features (k) as needed\n",
        "k = 1000\n",
        "selector = SelectKBest(chi2, k=k)\n",
        "X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "X_test_selected = selector.transform(X_test)\n",
        "\n",
        "# Display the shapes of the selected feature sets\n",
        "print(\"X_train_selected shape:\", X_train_selected.shape)\n",
        "print(\"X_test_selected shape:\", X_test_selected.shape)"
      ],
      "metadata": {
        "id": "tcoq_fCqDpaE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edaed6ab-986d-458f-a8c2-0f6f4c29e8d7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train_selected shape: (350, 1000)\n",
            "X_test_selected shape: (150, 1000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The shapes of `X_train_selected` and `X_test_selected` indicate the number of samples and the number of features in our training and testing sets, respectively. Here, `X_train_selected` has 350 samples with 1000 features, and `X_test_selected` has 150 samples with 1000 features.\n",
        "\n",
        "If the `test_size` parameter in the `train_test_split` function is set to 0.3, it means that 30% of the data should be allocated to the test set, and the remaining 70% to the training set. Let's verify if the distribution of samples in `X_train_selected` and `X_test_selected` aligns with this split:\n",
        "\n",
        "1. **Total number of samples**: The total is 350 (training) + 150 (testing) = 500 samples.\n",
        "2. **Expected distribution**:\n",
        "   - Training set: 70% of 500 = 0.7 * 500 = 350 samples\n",
        "   - Testing set: 30% of 500 = 0.3 * 500 = 150 samples\n",
        "\n",
        "The shapes `X_train_selected` (350, 1000) and `X_test_selected` (150, 1000) match the expected distribution with a test size of 30%. This means that the split has been done correctly, assigning 70% of the data to the training set and 30% to the test set while keeping the number of features consistent across both sets."
      ],
      "metadata": {
        "id": "xH2m3s5x7P8X"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slW7wA0L3Q3R"
      },
      "source": [
        "We have successfully performed feature selection, reducing the dimensionality of the data while retaining the most important features.\n",
        "\n",
        "\n",
        "## Step 4: Model Selection\n",
        "For sentiment analysis, you can use various machine learning algorithms like Logistic Regression, Naive Bayes, Support Vector Machines, or even deep learning models like LSTM or BERT. Since you're a beginner, let's start with a simple model like Logistic Regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Olqw9Aah3Wx6"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(random_state=42, class_weight='balanced')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"ex_3\"></a>\n",
        "## Exercise 3\n",
        "\n",
        "What does the random_state (parameter of the LogisticRegression) represent?\n",
        "\n",
        "**Answer**:\n",
        "\n",
        "The `random_state` parameter in `LogisticRegression` sets the seed for the random number generator, ensuring reproducibility of results by controlling randomness in the algorithm's execution, like data shuffling and coefficient initialization.\n",
        "\n",
        "By setting a `random_state`, the algorithm is instructed to start from the same point each time it runs, which ensures that if we run the same code again with the same data and the same `random_state`, we will get the exact same output.\n",
        "\n",
        "If we don't set a `random_state`, or if we set it to `None`, each run could produce slightly different results due to the randomness involved in the algorithm's execution. This could make debugging or replicating results challenging. Therefore, setting a `random_state` is a good practice when we need to ensure that our results are repeatable.\n"
      ],
      "metadata": {
        "id": "-eaR-mvhTpwp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KTqnTiB3leR"
      },
      "source": [
        "## Step 5: Training the Model\n",
        "\n",
        "Now that we have initialized our Logistic Regression model, it's time to train it on the selected features from the training dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "yFqbF79I3sFJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "675b2afd-5994-4486-eb4a-a2129a8ab2c3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(class_weight='balanced', random_state=42)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, random_state=42)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "\n",
        "# Train the Logistic Regression model on the selected features\n",
        "model.fit(X_train_selected, y_train)\n",
        "\n",
        "# We can now proceed to Step 7: Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtEOm5Y630p5"
      },
      "source": [
        "## Step 6: Model Evaluation\n",
        "\n",
        "In this step, we'll evaluate the performance of the trained Logistic Regression model using the testing data.\n",
        "\n",
        "- We import necessary metrics from `sklearn.metrics` such as `accuracy_score`, `classification_report`, and `confusion_matrix`.\n",
        "- We use the trained model to predict sentiment labels (`y_pred`) for the test data (`X_test_selected`).\n",
        "- We calculate the accuracy of the model by comparing the predicted labels to the true labels.\n",
        "- We display a classification report that includes precision, recall, F1-score, and support for both positive and negative sentiment classes.\n",
        "- We display a confusion matrix to visualize the true positive, true negative, false positive, and false negative predictions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4eNG5rY5323C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9954327d-ef05-4c15-f9ef-abf59354878b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8266666666666667\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.22      0.28        23\n",
            "           1       0.87      0.94      0.90       127\n",
            "\n",
            "    accuracy                           0.83       150\n",
            "   macro avg       0.63      0.58      0.59       150\n",
            "weighted avg       0.79      0.83      0.81       150\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[  5  18]\n",
            " [  8 119]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "# Predict sentiment labels for the test data\n",
        "y_pred = model.predict(X_test_selected)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Display a classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Display a confusion matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"ex_4\"></a>\n",
        "## Exercise 4\n",
        "\n",
        "- Compare the Results with the new data split with the results of the actual split."
      ],
      "metadata": {
        "id": "VUCBrmL4UASm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 80/20 Data split\n",
        "\n",
        "# Import necessary libraries for data pre-processing\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Remove any rows with missing values\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Encode the 'sentiments' column (positive/negative) to numerical values (0/1)\n",
        "le = LabelEncoder()\n",
        "df['sentiments'] = le.fit_transform(df['sentiments'])\n",
        "\n",
        "# Text data preprocessing using TF-IDF vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
        "X = tfidf_vectorizer.fit_transform(df['review_body']).toarray()\n",
        "y = df['sentiments'].values\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Display the shapes of the resulting data\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n",
        "\n",
        "k = 1000\n",
        "selector = SelectKBest(chi2, k=k)\n",
        "X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "X_test_selected = selector.transform(X_test)\n",
        "\n",
        "# Display the shapes of the selected feature sets\n",
        "print(\"X_train_selected shape:\", X_train_selected.shape)\n",
        "print(\"X_test_selected shape:\", X_test_selected.shape)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(random_state=42, class_weight='balanced')\n",
        "\n",
        "# Train the Logistic Regression model on the selected features\n",
        "model.fit(X_train_selected, y_train)\n",
        "\n",
        "# Predict sentiment labels for the test data\n",
        "y_pred = model.predict(X_test_selected)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Display a classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Display a confusion matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZqPm_ZXMJn3",
        "outputId": "fd7271dd-54ae-4680-9bf8-508768469e01"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (400, 3466)\n",
            "X_test shape: (100, 3466)\n",
            "y_train shape: (400,)\n",
            "y_test shape: (100,)\n",
            "X_train_selected shape: (400, 1000)\n",
            "X_test_selected shape: (100, 1000)\n",
            "Accuracy: 0.74\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.12      0.14      0.13        14\n",
            "           1       0.86      0.84      0.85        86\n",
            "\n",
            "    accuracy                           0.74       100\n",
            "   macro avg       0.49      0.49      0.49       100\n",
            "weighted avg       0.75      0.74      0.75       100\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 2 12]\n",
            " [14 72]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here\n",
        "# 70/30 Data split\n",
        "\n",
        "# Import necessary libraries for data pre-processing\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Remove any rows with missing values\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Encode the 'sentiments' column (positive/negative) to numerical values (0/1)\n",
        "le = LabelEncoder()\n",
        "df['sentiments'] = le.fit_transform(df['sentiments'])\n",
        "\n",
        "# Text data preprocessing using TF-IDF vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
        "X = tfidf_vectorizer.fit_transform(df['review_body']).toarray()\n",
        "y = df['sentiments'].values\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Display the shapes of the resulting data\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n",
        "\n",
        "k = 1000\n",
        "selector = SelectKBest(chi2, k=k)\n",
        "X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "X_test_selected = selector.transform(X_test)\n",
        "\n",
        "# Display the shapes of the selected feature sets\n",
        "print(\"X_train_selected shape:\", X_train_selected.shape)\n",
        "print(\"X_test_selected shape:\", X_test_selected.shape)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model1 = LogisticRegression(random_state=42, class_weight='balanced')\n",
        "\n",
        "# Train the Logistic Regression model on the selected features\n",
        "model1.fit(X_train_selected, y_train)\n",
        "\n",
        "# Predict sentiment labels for the test data\n",
        "y_pred1 = model.predict(X_test_selected)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred1)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Display a classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred1))\n",
        "\n",
        "# Display a confusion matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred1))"
      ],
      "metadata": {
        "id": "pY-p78AhRVCi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "291d13e9-a1db-4811-8300-335d6d6bdf3e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (350, 3466)\n",
            "X_test shape: (150, 3466)\n",
            "y_train shape: (350,)\n",
            "y_test shape: (150,)\n",
            "X_train_selected shape: (350, 1000)\n",
            "X_test_selected shape: (150, 1000)\n",
            "Accuracy: 0.8266666666666667\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.22      0.28        23\n",
            "           1       0.87      0.94      0.90       127\n",
            "\n",
            "    accuracy                           0.83       150\n",
            "   macro avg       0.63      0.58      0.59       150\n",
            "weighted avg       0.79      0.83      0.81       150\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[  5  18]\n",
            " [  8 119]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comparison result:-**\n",
        "\n",
        "The results from the two different data splits show some differences in the model's performance:\n",
        "\n",
        "**Accuracy:**\n",
        "- 80/20 Split: The accuracy was 0.74.\n",
        "- 70/30 Split: The accuracy improved to approximately 0.83.\n",
        "\n",
        "**Precision, Recall, and F1-Score:**\n",
        "- For class 0 (likely the minority class), both precision and recall increased in the 70/30 split compared to the 80/20 split, indicating a better ability to correctly identify and classify instances of class 0.\n",
        "- For class 1, there's a slight increase in recall, indicating better identification of positive class 1 instances in the 70/30 split. Precision remains high in both splits.\n",
        "\n",
        "**Confusion Matrix:**\n",
        "- The number of true positives for class 0 increased from 2 to 5, and false negatives decreased from 12 to 18 when changing from an 80/20 split to a 70/30 split, showing an improved but still challenged performance on class 0.\n",
        "- For class 1, the model shows a strong performance in both splits, with a notable increase in true positives from 72 to 119 and a decrease in false negatives from 14 to 8.\n",
        "\n",
        "**Analysis:**\n",
        "- The improved accuracy in the 70/30 split suggests that having a larger test set provided a more robust evaluation of the model, capturing its performance more accurately.\n",
        "- The changes in precision and recall for class 0 in the 70/30 split indicate that the model is getting slightly better at correctly identifying instances of the minority class when it has more test data to work with.\n",
        "- The improvement in class 1's recall suggests that with more test data, the model is better able to generalize its predictions for the majority class.\n",
        "\n",
        "Overall, the 70/30 split seems to provide a better balance for training and evaluating the model, giving it more data to test on and a better understanding of its generalization capabilities. However, the performance on class 0 remains a challenge, indicating potential issues with class imbalance or feature representation that might need further tuning or adjustment."
      ],
      "metadata": {
        "id": "3N9SGZK2L59D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"ex_5\"></a>\n",
        "## Exercise 5\n",
        "\n",
        "Do different training and testing sizes impact the model's learning and response to new data?"
      ],
      "metadata": {
        "id": "JOQTfrhqUKiw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer**: Write your answer here\n",
        "\n",
        "Yes, different training and testing sizes can significantly impact a model's learning and its response to new data:\n",
        "\n",
        "1. **Training Size Impact:** Larger training sets provide more data points for the model to learn from, capturing a broader range of patterns. However, if the training data is too large, it could introduce noise or irrelevant patterns, potentially leading to overfitting or increased training time without proportional gains in performance.\n",
        "\n",
        "2. **Testing Size Impact:** The size of the testing set affects the reliability of the model's performance evaluation. A larger testing set can give a more robust and reliable estimate of the model's performance on unseen data. Conversely, a smaller testing set might not fully capture the model's effectiveness and could lead to a less reliable assessment of its generalization capabilities.\n",
        "\n",
        "In conclusion, the division between training and testing data should be made thoughtfully, balancing the need for a model that learns well (larger training set) and the need to accurately assess its generalization to new data (sufficiently large testing set)."
      ],
      "metadata": {
        "id": "W15I941KUQWx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7L7dQj714Ckw"
      },
      "source": [
        "## Step 7: Hyperparameter Tuning\n",
        "\n",
        "In this step, we'll perform hyperparameter tuning to optimize the Logistic Regression model's performance. We can search for the best hyperparameters using techniques like Grid Search or Random Search.\n",
        "\n",
        "- We import `GridSearchCV` from `sklearn.model_selection`.\n",
        "- We define a grid of hyperparameters to search, including 'C' (regularization parameter) and 'max_iter' (maximum iterations).\n",
        "- We initialize Grid Search with cross-validation (5-fold) to find the best hyperparameters.\n",
        "- The best hyperparameters are extracted using `grid_search.best_params_`.\n",
        "- We fit the tuned model with the best hyperparameters to the training data.\n",
        "- Finally, we evaluate the tuned model's accuracy on the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "OX7ebRMa4GBT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68bd09dc-b920-4b2f-fdb1-8ba132b2c5d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "Best Hyperparameters: {'C': 100, 'max_iter': 100}\n",
            "Tuned Model Accuracy: 0.84\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define hyperparameters to search\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],  # Regularization parameters\n",
        "    'max_iter': [100, 200, 300]  # Maximum number of iterations\n",
        "}\n",
        "\n",
        "# Initialize Grid Search with cross-validation (5-fold)\n",
        "grid_search = GridSearchCV(LogisticRegression(random_state=42), param_grid, cv=5, verbose=1, n_jobs=-1)\n",
        "\n",
        "# Fit the Grid Search to the data\n",
        "grid_search.fit(X_train_selected, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "# Evaluate the model with the best hyperparameters\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred_tuned = best_model.predict(X_test_selected)\n",
        "\n",
        "# Calculate the accuracy of the tuned model\n",
        "accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n",
        "print(\"Tuned Model Accuracy:\", accuracy_tuned)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"ex_6\"></a>\n",
        "## Exercise 6\n",
        "\n",
        "- What is GridSearchCV used for?\n",
        "- What are hyperparameters?\n",
        "- Does the model give better results after hyperparameters ?"
      ],
      "metadata": {
        "id": "FaJSToyvUbBp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer**:\n",
        "\n",
        "**What is GridSearchCV used for?**\n",
        "\n",
        "GridSearchCV systematically searches through a specified range of hyperparameter values, performing cross-validation to determine the combination that yields the best model performance.It automates the process of finding the most optimal settings for a machine learning model.\n",
        "\n",
        "\n",
        "\n",
        "**What are hyperparameters?**\n",
        "\n",
        "Hyperparameters are the configuration settings of a model that are set prior to training and control the model's learning process and structure.They control the learning process itself (e.g., the complexity of the model, how fast it learns, etc.) and can significantly impact the performance of the model. Examples include the learning rate, kernel parameters in an SVM, depth for a decision tree, and regularization strength in logistic regression.\n",
        "\n",
        "\n",
        "\n",
        "**Does the model give better results after hyperparameter tuning?**\n",
        "\n",
        "Yes, by finding the optimal hyperparameters, the model is usually better tuned to the data, improving its performance and generalization ability. The whole point of hyperparameter tuning is to find the most optimal hyperparameter settings for our model relative to our data and the problem we solving. The optimal hyperparameters can improve the model's ability to generalise from the training data to unseen data, thus enhancing its overall performance."
      ],
      "metadata": {
        "id": "zhZdGMyeUk4J"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Axwiyok4PkA"
      },
      "source": [
        "It appears that the hyperparameter tuning did not significantly improve the model's accuracy in this case. The accuracy remains at 0.86.\n",
        "\n",
        "## Step 8: Cross Validation\n",
        "\n",
        "We'll use cross-validation to estimate how well the model will perform on unseen data and check if the model's performance is consistent across different folds of the data.\n",
        "\n",
        "- We import `cross_val_score` from `sklearn.model_selection`.\n",
        "- We perform 5-fold cross-validation on the tuned model (`best_model`) using the training data (`X_train_selected` and `y_train`).\n",
        "- We calculate the mean cross-validation accuracy to get a more robust estimate of the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries for data pre-processing\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Remove any rows with missing values\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Encode the 'sentiments' column (positive/negative) to numerical values (0/1)\n",
        "le = LabelEncoder()\n",
        "df['sentiments'] = le.fit_transform(df['sentiments'])\n",
        "\n",
        "# Text data preprocessing using TF-IDF vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
        "X = tfidf_vectorizer.fit_transform(df['review_body']).toarray()\n",
        "y = df['sentiments'].values\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Display the shapes of the resulting data\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n",
        "\n",
        "k = 1000\n",
        "selector = SelectKBest(chi2, k=k)\n",
        "X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "X_test_selected = selector.transform(X_test)\n",
        "\n",
        "# Define hyperparameters to search\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],  # Regularization parameters\n",
        "    'max_iter': [100, 200, 300]  # Maximum number of iterations\n",
        "}\n",
        "\n",
        "# Initialize Grid Search with cross-validation (5-fold)\n",
        "grid_search = GridSearchCV(LogisticRegression(random_state=42), param_grid, cv=5, verbose=1, n_jobs=-1)\n",
        "\n",
        "# Fit the Grid Search to the data\n",
        "grid_search.fit(X_train_selected, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "# Evaluate the model with the best hyperparameters\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred_tuned = best_model.predict(X_test_selected)\n",
        "\n",
        "\n",
        "# Perform 5-fold cross-validation on the tuned model\n",
        "cv_scores = cross_val_score(best_model, X_train_selected, y_train, cv=5)\n",
        "\n",
        "# Calculate and display the mean cross-validation accuracy\n",
        "mean_cv_accuracy = np.mean(cv_scores)\n",
        "print(\"Mean Cross-Validation Accuracy for 80/20 train-test data split:\", mean_cv_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmKCuuv-Q1nV",
        "outputId": "abfdc297-db2d-4ee5-8b97-d9d06a786365"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (400, 3466)\n",
            "X_test shape: (100, 3466)\n",
            "y_train shape: (400,)\n",
            "y_test shape: (100,)\n",
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "Best Hyperparameters: {'C': 100, 'max_iter': 100}\n",
            "Mean Cross-Validation Accuracy for 80/20 train-test data split: 0.79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries for data pre-processing\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Remove any rows with missing values\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Encode the 'sentiments' column (positive/negative) to numerical values (0/1)\n",
        "le = LabelEncoder()\n",
        "df['sentiments'] = le.fit_transform(df['sentiments'])\n",
        "\n",
        "# Text data preprocessing using TF-IDF vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
        "X = tfidf_vectorizer.fit_transform(df['review_body']).toarray()\n",
        "y = df['sentiments'].values\n",
        "\n",
        "# Split the data into training and testing sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Display the shapes of the resulting data\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n",
        "\n",
        "k = 1000\n",
        "selector = SelectKBest(chi2, k=k)\n",
        "X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "X_test_selected = selector.transform(X_test)\n",
        "\n",
        "# Define hyperparameters to search\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],  # Regularization parameters\n",
        "    'max_iter': [100, 200, 300]  # Maximum number of iterations\n",
        "}\n",
        "\n",
        "# Initialize Grid Search with cross-validation (5-fold)\n",
        "grid_search = GridSearchCV(LogisticRegression(random_state=42), param_grid, cv=5, verbose=1, n_jobs=-1)\n",
        "\n",
        "# Fit the Grid Search to the data\n",
        "grid_search.fit(X_train_selected, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "# Evaluate the model with the best hyperparameters\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred_tuned = best_model.predict(X_test_selected)\n",
        "\n",
        "\n",
        "# Perform 5-fold cross-validation on the tuned model\n",
        "cv_scores = cross_val_score(best_model, X_train_selected, y_train, cv=5)\n",
        "\n",
        "# Calculate and display the mean cross-validation accuracy\n",
        "mean_cv_accuracy = np.mean(cv_scores)\n",
        "print(\"Mean Cross-Validation Accuracy for 70/30 train-test data split:\", mean_cv_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWQXLYZTRLyt",
        "outputId": "2b6705c3-cac3-4810-cf11-00120f2a024a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (350, 3466)\n",
            "X_test shape: (150, 3466)\n",
            "y_train shape: (350,)\n",
            "y_test shape: (150,)\n",
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "Best Hyperparameters: {'C': 100, 'max_iter': 100}\n",
            "Mean Cross-Validation Accuracy for 70/30 train-test data split: 0.7885714285714285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ezr_YRmn4VkI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a39c2ce8-6761-4500-ffdd-edd1db8ba4d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Cross-Validation Accuracy: 0.7885714285714285\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Perform 5-fold cross-validation on the tuned model\n",
        "cv_scores = cross_val_score(best_model, X_train_selected, y_train, cv=5)\n",
        "\n",
        "# Calculate and display the mean cross-validation accuracy\n",
        "mean_cv_accuracy = np.mean(cv_scores)\n",
        "print(\"Mean Cross-Validation Accuracy:\", mean_cv_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"ex_7\"></a>\n",
        "## Exercise 7\n",
        "\n",
        "- What is Cross Validation used for?\n",
        "- Compare the new Validation score (with the new training and testing size)\n",
        "- What do you conclude ?"
      ],
      "metadata": {
        "id": "SaZiGRevUrOG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer**:"
      ],
      "metadata": {
        "id": "9zifBq1xVJ5b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*1. What is Cross Validation used for?*\n",
        "\n",
        "  Cross-validation is used to evaluate the generalizability of a model by dividing the data into several subsets, training the model on some subsets while validating it on the remaining ones. This process is repeated multiple times, helping to ensure that the model's performance is consistent across different subsets of the data, reducing the risk of overfitting and providing a more robust estimate of the model's performance on unseen data."
      ],
      "metadata": {
        "id": "QJt4DSi7S5f2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. *Compare the new Validation score (with the new training and testing size)*\n",
        "\n",
        "\n",
        "When comparing the mean cross-validation accuracies for different train-test splits, we are observing how altering the amount of training and testing data impacts the model's ability to generalize.\n",
        "\n",
        "1. **80/20 Split:**\n",
        "   - Mean Cross-Validation Accuracy: 0.79\n",
        "   - The model had more data to train on, potentially allowing it to learn more comprehensive patterns from the data.\n",
        "\n",
        "2. **70/30 Split:**\n",
        "   - Mean Cross-Validation Accuracy: 0.7885714285714285\n",
        "   - Despite having less training data compared to the 80/20 split, the model's performance is quite similar, indicating good generalization.\n",
        "\n"
      ],
      "metadata": {
        "id": "Mn1jucAiS6Wc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*3. What do you conclude?*\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- The slight difference in cross-validation accuracy between the two splits suggests that the model is relatively stable across different amounts of training data.\n",
        "- The consistency in best hyperparameters (`'C': 100, 'max_iter': 100`) for both splits implies that the model's optimal configuration is robust to changes in the train-test ratio.\n",
        "- Since the performance is similar across both splits, it indicates that the model, with the given hyperparameters, is not heavily dependent on the exact proportion of training to testing data, at least within the range tested (80/20 vs. 70/30).\n",
        "\n",
        "This analysis helps in understanding the trade-off between training with more data and having a larger set to validate the model's generalizability."
      ],
      "metadata": {
        "id": "KLkzA1QBTYmu"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}