{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Problem statement: Create a classification model for the Fashion MNIST\n",
        "\n",
        "The objective is to create a classification model for the Fashion MNIST dataset using a Multi-Layer Perceptron (MLP).\n",
        "\n",
        "We'll follow these steps:\n",
        "\n",
        "### 1. Data Preprocessing\n",
        "- **Loading the Data**: Fashion MNIST is a dataset of Zalando's article images, with 60,000 training samples and 10,000 test samples. Each sample is a 28x28 grayscale image, associated with a label from 10 classes.\n",
        "- **Normalization**: We normalize the pixel values (ranging from 0 to 255) to a scale of 0 to 1. This improves the training efficiency.\n",
        "- **Reshaping for MLP**: Since we are using an MLP, we need to reshape the 28x28 images into a flat array of 784 pixels.\n",
        "\n",
        "### 2. Building the MLP Model\n",
        "- **Dense Layers**: These are fully connected neural layers. The first layer needs to know the input shape (784 in this case).\n",
        "- **Activation Functions**: 'ReLU' is used for non-linear transformations. The final layer uses 'softmax' for a probability distribution over 10 classes.\n",
        "\n",
        "### 3. Compiling the Model\n",
        "- **Optimizer**: 'Adam' is a popular choice for its adaptive learning rate properties.\n",
        "- **Loss Function**: 'sparse_categorical_crossentropy' is suitable for multi-class classification problems.\n",
        "- **Metrics**: We'll use 'accuracy' to understand the performance.\n",
        "\n",
        "### 4. Training the Model\n",
        "- We train the model using the `fit` method, specifying epochs and batch size.\n",
        "\n",
        "### 5. Evaluating the Model\n",
        "- The `evaluate` method is used to test the model on the test set.\n",
        "\n",
        "The notebook contains one exercise in total:\n",
        "\n",
        "* [Exercise 1](#ex_1)"
      ],
      "metadata": {
        "id": "7-1RI2LOhzW2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKlxud86hcIh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc52b5c1-e08b-4315-c673-30d529f13333"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "938/938 [==============================] - 6s 4ms/step - loss: 0.5203 - accuracy: 0.8191\n",
            "Epoch 2/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.3827 - accuracy: 0.8642\n",
            "Epoch 3/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.3443 - accuracy: 0.8758\n",
            "Epoch 4/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.3225 - accuracy: 0.8828\n",
            "Epoch 5/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.3025 - accuracy: 0.8894\n",
            "Epoch 6/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2891 - accuracy: 0.8943\n",
            "Epoch 7/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2759 - accuracy: 0.8985\n",
            "Epoch 8/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2637 - accuracy: 0.9032\n",
            "Epoch 9/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2545 - accuracy: 0.9059\n",
            "Epoch 10/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2484 - accuracy: 0.9082\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3272 - accuracy: 0.8839\n",
            "Test accuracy: 0.883899986743927\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load the dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Normalize the images to [0, 1]\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# Reshape data for MLP input\n",
        "train_images = train_images.reshape((-1, 28*28))\n",
        "test_images = test_images.reshape((-1, 28*28))\n",
        "\n",
        "# Build the MLP model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, activation='relu', input_shape=(784,)))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_images, train_labels, epochs=10, batch_size=64)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "\n",
        "print('Test accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To improve the model's accuracy on the Fashion MNIST dataset, we can experiment with various techniques. Here are some strategies:\n",
        "\n",
        "1. **Increase Model Complexity**: Add more layers or increase the number of neurons in each layer to capture more complex patterns in the data.\n",
        "\n",
        "2. **Regularization**: Implement dropout or L1/L2 regularization to reduce overfitting.\n",
        "\n",
        "3. **Advanced Optimizers**: Experiment with different optimizers like SGD or RMSprop.\n",
        "\n",
        "4. **Learning Rate Scheduling**: Adjust the learning rate during training.\n",
        "\n",
        "5. **Data Augmentation**: Although not typical for MLPs, slight modifications to the input data can make the model more robust.\n",
        "\n",
        "6. **Early Stopping**: Stop training when the validation accuracy stops improving.\n",
        "\n",
        "7. **Hyperparameter Tuning**: Experiment with different activation functions, batch sizes, and epochs.\n",
        "\n",
        "8. **Batch Normalization**: This can help in faster convergence and overall performance improvement.\n",
        "\n",
        "Let's modify the previous code to incorporate some of these strategies."
      ],
      "metadata": {
        "id": "yL1TJaEzjvVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Modified MLP model\n",
        "model = Sequential()\n",
        "model.add(Dense(256, activation='relu', input_shape=(784,)))\n",
        "model.add(BatchNormalization())  # Batch normalization layer\n",
        "model.add(Dropout(0.5))         # Dropout layer\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(BatchNormalization())  # Another batch normalization layer\n",
        "model.add(Dropout(0.5))         # Another dropout layer\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "# Train the model with validation split\n",
        "model.fit(train_images, train_labels, epochs=50, batch_size=64,\n",
        "          validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "\n",
        "print('Test accuracy:', test_acc)"
      ],
      "metadata": {
        "id": "GkitFh5aij7B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaf5aa78-1802-4e5f-835d-7496beec4cbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "750/750 [==============================] - 8s 5ms/step - loss: 0.7146 - accuracy: 0.7529 - val_loss: 0.4368 - val_accuracy: 0.8399\n",
            "Epoch 2/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.5313 - accuracy: 0.8109 - val_loss: 0.4211 - val_accuracy: 0.8447\n",
            "Epoch 3/50\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.4955 - accuracy: 0.8234 - val_loss: 0.4027 - val_accuracy: 0.8519\n",
            "Epoch 4/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.4742 - accuracy: 0.8324 - val_loss: 0.4019 - val_accuracy: 0.8558\n",
            "Epoch 5/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.4523 - accuracy: 0.8382 - val_loss: 0.3848 - val_accuracy: 0.8564\n",
            "Epoch 6/50\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.4477 - accuracy: 0.8410 - val_loss: 0.3740 - val_accuracy: 0.8611\n",
            "Epoch 7/50\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.4339 - accuracy: 0.8463 - val_loss: 0.3657 - val_accuracy: 0.8675\n",
            "Epoch 8/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.4387 - accuracy: 0.8425 - val_loss: 0.3745 - val_accuracy: 0.8621\n",
            "Epoch 9/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.4295 - accuracy: 0.8461 - val_loss: 0.3648 - val_accuracy: 0.8660\n",
            "Epoch 10/50\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.4159 - accuracy: 0.8512 - val_loss: 0.3870 - val_accuracy: 0.8537\n",
            "Epoch 11/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.4090 - accuracy: 0.8526 - val_loss: 0.3556 - val_accuracy: 0.8707\n",
            "Epoch 12/50\n",
            "750/750 [==============================] - 4s 6ms/step - loss: 0.4022 - accuracy: 0.8548 - val_loss: 0.3506 - val_accuracy: 0.8714\n",
            "Epoch 13/50\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.4011 - accuracy: 0.8556 - val_loss: 0.3412 - val_accuracy: 0.8782\n",
            "Epoch 14/50\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.3962 - accuracy: 0.8570 - val_loss: 0.3500 - val_accuracy: 0.8744\n",
            "Epoch 15/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.3910 - accuracy: 0.8585 - val_loss: 0.3525 - val_accuracy: 0.8697\n",
            "Epoch 16/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.3919 - accuracy: 0.8590 - val_loss: 0.3417 - val_accuracy: 0.8741\n",
            "Epoch 17/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.3888 - accuracy: 0.8590 - val_loss: 0.3511 - val_accuracy: 0.8733\n",
            "Epoch 18/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.3842 - accuracy: 0.8624 - val_loss: 0.3419 - val_accuracy: 0.8775\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3671 - accuracy: 0.8680\n",
            "Test accuracy: 0.8679999709129333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The test accuracy decreased slightly in this case. This outcome highlights an important aspect of machine learning: improvements in model architecture don't always lead to better performance, and sometimes simpler models can outperform more complex ones, especially on smaller datasets like Fashion MNIST.\n",
        "\n",
        "Here are a few additional steps you can take to try and improve the model's performance:\n",
        "\n",
        "1. **Adjust the Dropout Rate**: The dropout rate of 0.5 might be too high, causing the model to lose relevant information. Try reducing it to 0.3 or 0.2.\n",
        "\n",
        "2. **Fine-Tune the Model Complexity**: The addition of more neurons might have made the model too complex. Try reducing the number of neurons in the dense layers.\n",
        "\n",
        "3. **Experiment with Different Optimizers**: While Adam is a strong general-purpose optimizer, sometimes others like SGD (with a momentum) or RMSprop might yield better results for specific problems.\n",
        "\n",
        "4. **Modify the Learning Rate**: Adjusting the learning rate of the Adam optimizer could also lead to better results. A lower learning rate with more epochs can sometimes achieve better generalization.\n",
        "\n",
        "5. **Experiment with Batch Sizes**: Smaller or larger batch sizes can impact the model's ability to generalize and learn effectively.\n",
        "\n",
        "6. **Cross-Validation**: Instead of a single validation split, use k-fold cross-validation for a more robust estimate of model performance.\n",
        "\n",
        "Let's adjust the code with some of these suggestions."
      ],
      "metadata": {
        "id": "lGXeENRJjzVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust the model architecture and training parameters\n",
        "model = Sequential()\n",
        "model.add(Dense(128, activation='relu', input_shape=(784,)))\n",
        "model.add(Dropout(0.3))         # Reduced dropout rate\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.3))         # Reduced dropout rate\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model with a modified optimizer\n",
        "model.compile(optimizer='adam',  # You can experiment with learning rate here\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model with a different batch size\n",
        "model.fit(train_images, train_labels, epochs=50, batch_size=32,  # Smaller batch size\n",
        "          validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "\n",
        "print('Test accuracy:', test_acc)"
      ],
      "metadata": {
        "id": "T9z3khhbjrVp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7d82ff8-4e84-428c-8156-0dcc48fd6c96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1500/1500 [==============================] - 7s 4ms/step - loss: 0.6683 - accuracy: 0.7653 - val_loss: 0.4358 - val_accuracy: 0.8395\n",
            "Epoch 2/50\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.4756 - accuracy: 0.8294 - val_loss: 0.3996 - val_accuracy: 0.8545\n",
            "Epoch 3/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.4349 - accuracy: 0.8435 - val_loss: 0.3921 - val_accuracy: 0.8533\n",
            "Epoch 4/50\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.4143 - accuracy: 0.8495 - val_loss: 0.3618 - val_accuracy: 0.8633\n",
            "Epoch 5/50\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.3973 - accuracy: 0.8550 - val_loss: 0.3593 - val_accuracy: 0.8693\n",
            "Epoch 6/50\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.3794 - accuracy: 0.8635 - val_loss: 0.3537 - val_accuracy: 0.8683\n",
            "Epoch 7/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.3725 - accuracy: 0.8664 - val_loss: 0.3415 - val_accuracy: 0.8760\n",
            "Epoch 8/50\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.3601 - accuracy: 0.8680 - val_loss: 0.3307 - val_accuracy: 0.8792\n",
            "Epoch 9/50\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.3527 - accuracy: 0.8699 - val_loss: 0.3436 - val_accuracy: 0.8763\n",
            "Epoch 10/50\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.3466 - accuracy: 0.8745 - val_loss: 0.3542 - val_accuracy: 0.8698\n",
            "Epoch 11/50\n",
            "1500/1500 [==============================] - 5s 4ms/step - loss: 0.3390 - accuracy: 0.8756 - val_loss: 0.3306 - val_accuracy: 0.8816\n",
            "Epoch 12/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.3336 - accuracy: 0.8764 - val_loss: 0.3171 - val_accuracy: 0.8845\n",
            "Epoch 13/50\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.3283 - accuracy: 0.8787 - val_loss: 0.3322 - val_accuracy: 0.8841\n",
            "Epoch 14/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.3219 - accuracy: 0.8794 - val_loss: 0.3294 - val_accuracy: 0.8807\n",
            "Epoch 15/50\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.3271 - accuracy: 0.8794 - val_loss: 0.3320 - val_accuracy: 0.8801\n",
            "Epoch 16/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.3172 - accuracy: 0.8839 - val_loss: 0.3196 - val_accuracy: 0.8855\n",
            "Epoch 17/50\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.3171 - accuracy: 0.8826 - val_loss: 0.3410 - val_accuracy: 0.8780\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3709 - accuracy: 0.8687\n",
            "Test accuracy: 0.8687000274658203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The test accuracy has improved to 0.8778, which is a positive outcome. This result indicates that the adjustments made to the model architecture and training parameters were beneficial.\n",
        "\n",
        "However, achieving higher accuracy on a dataset like Fashion MNIST can be challenging, especially with a simple model like a Multi-Layer Perceptron (MLP). To potentially achieve even better results, consider the following additional steps:\n",
        "\n",
        "1. **Feature Engineering**: Although this is more limited with image data and MLPs, ensuring the input data is as informative and clean as possible is crucial.\n",
        "\n",
        "2. **Ensemble Methods**: Combine predictions from several models to improve accuracy. For example, train multiple MLPs with different architectures and average their predictions.\n",
        "\n",
        "3. **Convolutional Neural Networks (CNNs)**: For image data, CNNs are generally more effective than MLPs. They can capture spatial hierarchies in the data better due to their convolutional layers.\n",
        "\n",
        "4. **Hyperparameter Optimization**: Use techniques like grid search or random search to systematically explore different hyperparameter combinations.\n",
        "\n",
        "5. **Advanced Regularization Techniques**: Experiment with other regularization methods like L1 regularization or different dropout configurations.\n",
        "\n",
        "Let's adjust the code with some of these suggestions."
      ],
      "metadata": {
        "id": "rmWbPC5gmHZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "# Reshape data for CNN input\n",
        "train_images_cnn = train_images.reshape((-1, 28, 28, 1))\n",
        "test_images_cnn = test_images.reshape((-1, 28, 28, 1))\n",
        "\n",
        "# Build a simple CNN model\n",
        "cnn_model = Sequential()\n",
        "cnn_model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "cnn_model.add(MaxPooling2D((2, 2)))\n",
        "cnn_model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "cnn_model.add(MaxPooling2D((2, 2)))\n",
        "cnn_model.add(Flatten())\n",
        "cnn_model.add(Dense(64, activation='relu'))\n",
        "cnn_model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "cnn_model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "cnn_model.fit(train_images_cnn, train_labels, epochs=10, batch_size=64,\n",
        "              validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = cnn_model.evaluate(test_images_cnn, test_labels)\n",
        "\n",
        "print('CNN Test accuracy:', test_acc)"
      ],
      "metadata": {
        "id": "BKPmmHqImJEi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d2650e1-c4aa-4ee7-9efb-6059b034ebbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "750/750 [==============================] - 7s 6ms/step - loss: 0.5412 - accuracy: 0.8044 - val_loss: 0.4077 - val_accuracy: 0.8520\n",
            "Epoch 2/10\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.3562 - accuracy: 0.8721 - val_loss: 0.3331 - val_accuracy: 0.8782\n",
            "Epoch 3/10\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.3052 - accuracy: 0.8896 - val_loss: 0.3127 - val_accuracy: 0.8857\n",
            "Epoch 4/10\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.2734 - accuracy: 0.8999 - val_loss: 0.2828 - val_accuracy: 0.8967\n",
            "Epoch 5/10\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.2500 - accuracy: 0.9086 - val_loss: 0.2682 - val_accuracy: 0.9009\n",
            "Epoch 6/10\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.2320 - accuracy: 0.9144 - val_loss: 0.2668 - val_accuracy: 0.9064\n",
            "Epoch 7/10\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.2116 - accuracy: 0.9220 - val_loss: 0.2507 - val_accuracy: 0.9089\n",
            "Epoch 8/10\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1970 - accuracy: 0.9255 - val_loss: 0.2547 - val_accuracy: 0.9092\n",
            "Epoch 9/10\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.1807 - accuracy: 0.9322 - val_loss: 0.2654 - val_accuracy: 0.9082\n",
            "Epoch 10/10\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.1686 - accuracy: 0.9376 - val_loss: 0.2551 - val_accuracy: 0.9101\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.2722 - accuracy: 0.9049\n",
            "CNN Test accuracy: 0.9049000144004822\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"ex_1\"></a>\n",
        "## Exercise 1: Improve the accuracy of the MLP model\n",
        "1. Try different architectures and hyperparameters.\n",
        "2. Use regularization techniques like L1 or L2 regularization.\n",
        "3. Use dropout to reduce overfitting.\n",
        "\n",
        "Referans link: https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/"
      ],
      "metadata": {
        "id": "WACMOuHj3wp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.regularizers import l1, l2\n",
        "\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load the dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Normalize the images to [0, 1]\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# Reshape data for MLP input\n",
        "train_images = train_images.reshape((-1, 28*28))\n",
        "test_images = test_images.reshape((-1, 28*28))\n",
        "\n",
        "# Reshape data for CNN input\n",
        "train_images_cnn = train_images.reshape((-1, 28, 28, 1))\n",
        "test_images_cnn = test_images.reshape((-1, 28, 28, 1))\n",
        "\n",
        "# Build an improved CNN model with regularization and dropout\n",
        "cnn_model = Sequential()\n",
        "cnn_model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), kernel_regularizer=l2(0.001)))\n",
        "cnn_model.add(MaxPooling2D((2, 2)))\n",
        "cnn_model.add(Dropout(0.25))\n",
        "cnn_model.add(Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.001)))\n",
        "cnn_model.add(MaxPooling2D((2, 2)))\n",
        "cnn_model.add(Dropout(0.25))\n",
        "cnn_model.add(Flatten())\n",
        "cnn_model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.001)))\n",
        "cnn_model.add(Dropout(0.5))\n",
        "cnn_model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "cnn_model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Train the model with modified epochs -\n",
        "cnn_model.fit(train_images_cnn, train_labels, epochs=10, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = cnn_model.evaluate(test_images_cnn, test_labels)\n",
        "\n",
        "print('Improved CNN Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mbnks_hRdNBJ",
        "outputId": "a9ce2f10-b540-488e-df38-c21fc246385a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "750/750 [==============================] - 7s 5ms/step - loss: 0.9139 - accuracy: 0.7285 - val_loss: 0.6247 - val_accuracy: 0.8330\n",
            "Epoch 2/10\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.6476 - accuracy: 0.8156 - val_loss: 0.5467 - val_accuracy: 0.8553\n",
            "Epoch 3/10\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.5956 - accuracy: 0.8321 - val_loss: 0.4934 - val_accuracy: 0.8657\n",
            "Epoch 4/10\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.5609 - accuracy: 0.8470 - val_loss: 0.5012 - val_accuracy: 0.8635\n",
            "Epoch 5/10\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.5393 - accuracy: 0.8553 - val_loss: 0.4972 - val_accuracy: 0.8623\n",
            "Epoch 6/10\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.5300 - accuracy: 0.8572 - val_loss: 0.4487 - val_accuracy: 0.8840\n",
            "Epoch 7/10\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.5197 - accuracy: 0.8610 - val_loss: 0.4488 - val_accuracy: 0.8831\n",
            "Epoch 8/10\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.5110 - accuracy: 0.8650 - val_loss: 0.4518 - val_accuracy: 0.8839\n",
            "Epoch 9/10\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.5104 - accuracy: 0.8650 - val_loss: 0.4377 - val_accuracy: 0.8906\n",
            "Epoch 10/10\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.5045 - accuracy: 0.8674 - val_loss: 0.4332 - val_accuracy: 0.8898\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.4433 - accuracy: 0.8885\n",
            "Improved CNN Test accuracy: 0.8884999752044678\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Here's what we have changed:\n",
        "1. **Architecture**: Increased the depth of the network by adding more filters to the convolutional layers and more neurons in the dense layer before the output layer.\n",
        "2. **Regularization**: Added L2 regularization to the convolutional and dense layers to help prevent overfitting by penalizing large weights.\n",
        "3. **Dropout**: Included dropout layers after max pooling and before the output layer to reduce overfitting further by randomly setting a fraction of the input units to 0 at each update during training.\n",
        "\n",
        "With these modifications, the model may yield better generalization on the test set, which should be reflected in an improved test accuracy."
      ],
      "metadata": {
        "id": "qJWdMesmfhvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.regularizers import l1, l2\n",
        "\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load the dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Normalize the images to [0, 1]\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# Reshape data for MLP input\n",
        "train_images = train_images.reshape((-1, 28*28))\n",
        "test_images = test_images.reshape((-1, 28*28))\n",
        "\n",
        "# Reshape data for CNN input\n",
        "train_images_cnn = train_images.reshape((-1, 28, 28, 1))\n",
        "test_images_cnn = test_images.reshape((-1, 28, 28, 1))\n",
        "\n",
        "# Build an improved CNN model with regularization and dropout\n",
        "cnn_model = Sequential()\n",
        "cnn_model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), kernel_regularizer=l2(0.001)))\n",
        "cnn_model.add(MaxPooling2D((2, 2)))\n",
        "cnn_model.add(Dropout(0.25))\n",
        "cnn_model.add(Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.001)))\n",
        "cnn_model.add(MaxPooling2D((2, 2)))\n",
        "cnn_model.add(Dropout(0.25))\n",
        "cnn_model.add(Flatten())\n",
        "cnn_model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.001)))\n",
        "cnn_model.add(Dropout(0.5))\n",
        "cnn_model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "cnn_model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Train the model with more epochs -  # Increased epochs from 10 to 20\n",
        "cnn_model.fit(train_images_cnn, train_labels, epochs=20, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = cnn_model.evaluate(test_images_cnn, test_labels)\n",
        "\n",
        "print('Improved CNN Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hQmOodUfuw2",
        "outputId": "a0a5ed88-ccf6-4889-f8f2-048b2e835a04"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "750/750 [==============================] - 6s 6ms/step - loss: 0.8922 - accuracy: 0.7385 - val_loss: 0.6263 - val_accuracy: 0.8215\n",
            "Epoch 2/20\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.6505 - accuracy: 0.8110 - val_loss: 0.5540 - val_accuracy: 0.8489\n",
            "Epoch 3/20\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.5893 - accuracy: 0.8330 - val_loss: 0.5005 - val_accuracy: 0.8627\n",
            "Epoch 4/20\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.5587 - accuracy: 0.8435 - val_loss: 0.4740 - val_accuracy: 0.8753\n",
            "Epoch 5/20\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.5434 - accuracy: 0.8507 - val_loss: 0.4643 - val_accuracy: 0.8776\n",
            "Epoch 6/20\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.5251 - accuracy: 0.8574 - val_loss: 0.4571 - val_accuracy: 0.8821\n",
            "Epoch 7/20\n",
            "750/750 [==============================] - 4s 6ms/step - loss: 0.5204 - accuracy: 0.8602 - val_loss: 0.4548 - val_accuracy: 0.8815\n",
            "Epoch 8/20\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.5089 - accuracy: 0.8643 - val_loss: 0.4424 - val_accuracy: 0.8847\n",
            "Epoch 9/20\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.5049 - accuracy: 0.8655 - val_loss: 0.4327 - val_accuracy: 0.8882\n",
            "Epoch 10/20\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.5006 - accuracy: 0.8667 - val_loss: 0.4337 - val_accuracy: 0.8870\n",
            "Epoch 11/20\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.4938 - accuracy: 0.8696 - val_loss: 0.4460 - val_accuracy: 0.8858\n",
            "Epoch 12/20\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.4929 - accuracy: 0.8689 - val_loss: 0.4268 - val_accuracy: 0.8912\n",
            "Epoch 13/20\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.4887 - accuracy: 0.8719 - val_loss: 0.4244 - val_accuracy: 0.8921\n",
            "Epoch 14/20\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.4855 - accuracy: 0.8721 - val_loss: 0.4222 - val_accuracy: 0.8911\n",
            "Epoch 15/20\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.4842 - accuracy: 0.8720 - val_loss: 0.4349 - val_accuracy: 0.8848\n",
            "Epoch 16/20\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.4797 - accuracy: 0.8742 - val_loss: 0.4267 - val_accuracy: 0.8909\n",
            "Epoch 17/20\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.4777 - accuracy: 0.8756 - val_loss: 0.4120 - val_accuracy: 0.8985\n",
            "Epoch 18/20\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.4725 - accuracy: 0.8777 - val_loss: 0.4191 - val_accuracy: 0.8942\n",
            "Epoch 19/20\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.4710 - accuracy: 0.8768 - val_loss: 0.4154 - val_accuracy: 0.8955\n",
            "Epoch 20/20\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.4712 - accuracy: 0.8782 - val_loss: 0.4399 - val_accuracy: 0.8827\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.4489 - accuracy: 0.8810\n",
            "Improved CNN Test accuracy: 0.8809999823570251\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's what we have changed:\n",
        "\n",
        "**Training**: Increased the number of epochs to give the model more time to learn.\n"
      ],
      "metadata": {
        "id": "MdBjLEclgF0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.regularizers import l1, l2\n",
        "\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load the dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Normalize the images to [0, 1]\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# Reshape data for MLP input\n",
        "train_images = train_images.reshape((-1, 28*28))\n",
        "test_images = test_images.reshape((-1, 28*28))\n",
        "\n",
        "# Reshape data for CNN input\n",
        "train_images_cnn = train_images.reshape((-1, 28, 28, 1))\n",
        "test_images_cnn = test_images.reshape((-1, 28, 28, 1))\n",
        "\n",
        "# Build an improved CNN model with modified regularization\n",
        "cnn_model = Sequential()\n",
        "cnn_model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), kernel_regularizer=l2(0.0001))) # Modified regularization value from 0.001 to 0.0001\n",
        "cnn_model.add(MaxPooling2D((2, 2)))\n",
        "cnn_model.add(Dropout(0.25))\n",
        "cnn_model.add(Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.0001))) # Modified regularization value from 0.001 to 0.0001\n",
        "cnn_model.add(MaxPooling2D((2, 2)))\n",
        "cnn_model.add(Dropout(0.25))\n",
        "cnn_model.add(Flatten())\n",
        "cnn_model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.0001))) # Modified regularization value from 0.001 to 0.0001\n",
        "cnn_model.add(Dropout(0.5))\n",
        "cnn_model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "cnn_model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "cnn_model.fit(train_images_cnn, train_labels, epochs=20, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = cnn_model.evaluate(test_images_cnn, test_labels)\n",
        "\n",
        "print('Improved CNN Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2rQryVIghMx",
        "outputId": "a1101764-c9e1-4f62-b1c2-0882090b0da3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "750/750 [==============================] - 5s 5ms/step - loss: 0.7549 - accuracy: 0.7331 - val_loss: 0.4857 - val_accuracy: 0.8359\n",
            "Epoch 2/20\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.5305 - accuracy: 0.8192 - val_loss: 0.4158 - val_accuracy: 0.8622\n",
            "Epoch 3/20\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.4793 - accuracy: 0.8413 - val_loss: 0.3943 - val_accuracy: 0.8694\n",
            "Epoch 4/20\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.4454 - accuracy: 0.8536 - val_loss: 0.3640 - val_accuracy: 0.8826\n",
            "Epoch 5/20\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.4242 - accuracy: 0.8634 - val_loss: 0.3561 - val_accuracy: 0.8864\n",
            "Epoch 6/20\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.4118 - accuracy: 0.8703 - val_loss: 0.3458 - val_accuracy: 0.8953\n",
            "Epoch 7/20\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.4003 - accuracy: 0.8766 - val_loss: 0.3369 - val_accuracy: 0.8978\n",
            "Epoch 8/20\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.3937 - accuracy: 0.8787 - val_loss: 0.3389 - val_accuracy: 0.8972\n",
            "Epoch 9/20\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.3815 - accuracy: 0.8831 - val_loss: 0.3223 - val_accuracy: 0.9016\n",
            "Epoch 10/20\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.3813 - accuracy: 0.8838 - val_loss: 0.3385 - val_accuracy: 0.8969\n",
            "Epoch 11/20\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.3697 - accuracy: 0.8895 - val_loss: 0.3263 - val_accuracy: 0.9034\n",
            "Epoch 12/20\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.3667 - accuracy: 0.8882 - val_loss: 0.3283 - val_accuracy: 0.9023\n",
            "Epoch 13/20\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.3619 - accuracy: 0.8925 - val_loss: 0.3236 - val_accuracy: 0.9028\n",
            "Epoch 14/20\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.3597 - accuracy: 0.8946 - val_loss: 0.3208 - val_accuracy: 0.9063\n",
            "Epoch 15/20\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.3546 - accuracy: 0.8967 - val_loss: 0.3175 - val_accuracy: 0.9109\n",
            "Epoch 16/20\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.3502 - accuracy: 0.8986 - val_loss: 0.3236 - val_accuracy: 0.9091\n",
            "Epoch 17/20\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.3485 - accuracy: 0.8999 - val_loss: 0.3119 - val_accuracy: 0.9107\n",
            "Epoch 18/20\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.3510 - accuracy: 0.8969 - val_loss: 0.3202 - val_accuracy: 0.9061\n",
            "Epoch 19/20\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.3478 - accuracy: 0.8992 - val_loss: 0.3121 - val_accuracy: 0.9122\n",
            "Epoch 20/20\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.3454 - accuracy: 0.9006 - val_loss: 0.3089 - val_accuracy: 0.9129\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.3216 - accuracy: 0.9078\n",
            "Improved CNN Test accuracy: 0.907800018787384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's what we have changed - ***Changed the l2 regularization value from 0.001 to 0.0001***\n",
        "\n",
        "Adjusting the regularization value from 0.001 to 0.0001 and observing an improvement in test accuracy suggests that the previous regularization might have been too strong for our model. Regularization is a technique used to prevent overfitting by discouraging complex models. However, if the regularization is too strong, it can lead to underfitting, where the model is not complex enough to capture the underlying patterns in the data.\n",
        "\n",
        "Here's why reducing the regularization strength might have led to improved accuracy:\n",
        "\n",
        "1. **Balance between Bias and Variance:** A higher regularization term (0.001 in your initial model) adds more bias and reduces variance, which can be good for a model that overfits. However, if the model was not overfitting, reducing this bias (by lowering the regularization term to 0.0001) might allow the model to better capture the signal in the training data without causing a significant increase in variance, thus improving accuracy.\n",
        "\n",
        "2. **Optimal Complexity:** Neural networks learn by adjusting their weights. If the penalty for having larger weights is too severe, the network might not be able to learn effectively from the training data. By reducing the regularization value, you allow the weights to grow larger if doing so improves the model's performance on the training data, which may result in a model that generalizes better.\n",
        "\n",
        "3. **Model Capacity:** With less regularization, the model can utilize more of its capacity to learn from the data. If the initial capacity was too constrained by regularization, reducing it slightly might provide the model with just enough flexibility to improve its predictive performance.\n",
        "\n",
        "4. **Learning Dynamics:** Regularization affects the learning process. A smaller regularization value might have provided a better gradient flow, helping the optimization process to find a better set of weights, especially in a deep network where avoiding vanishing or exploding gradients is crucial.\n",
        "\n",
        "In practice, finding the right amount of regularization is a part of model tuning and usually requires experimentation. It's always a trade-off between enough regularization to prevent overfitting and not too much to avoid underfitting. The improvement in test accuracy indicates that the latter value (0.0001) was a better fit for our model's complexity and the data it was learning from."
      ],
      "metadata": {
        "id": "mCiAKBBmigKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have thus improved the accuracy from 0.9049000144004822 to **0.907800018787384**"
      ],
      "metadata": {
        "id": "hYmUVcDMjD4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.regularizers import l1, l2\n",
        "\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load the dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Normalize the images to [0, 1]\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# Reshape data for MLP input\n",
        "train_images = train_images.reshape((-1, 28*28))\n",
        "test_images = test_images.reshape((-1, 28*28))\n",
        "\n",
        "# Reshape data for CNN input\n",
        "train_images_cnn = train_images.reshape((-1, 28, 28, 1))\n",
        "test_images_cnn = test_images.reshape((-1, 28, 28, 1))\n",
        "\n",
        "# Build CNN model with regularization and dropout\n",
        "cnn_model = Sequential()\n",
        "cnn_model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), kernel_regularizer=l2(0.0001)))\n",
        "cnn_model.add(MaxPooling2D((2, 2)))\n",
        "cnn_model.add(Dropout(0.25))\n",
        "cnn_model.add(Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.0001)))\n",
        "cnn_model.add(MaxPooling2D((2, 2)))\n",
        "cnn_model.add(Dropout(0.25))\n",
        "cnn_model.add(Flatten())\n",
        "cnn_model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.0001)))\n",
        "cnn_model.add(Dropout(0.5))\n",
        "cnn_model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "cnn_model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Train the model with more epochs -  # Increased epochs from 20 to 25\n",
        "cnn_model.fit(train_images_cnn, train_labels, epochs=25, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = cnn_model.evaluate(test_images_cnn, test_labels)\n",
        "\n",
        "print('Improved CNN Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-6vvwBOj6EX",
        "outputId": "6cd22fe3-6426-4b15-f0d8-e26ca00807c0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "750/750 [==============================] - 6s 6ms/step - loss: 0.7382 - accuracy: 0.7427 - val_loss: 0.4693 - val_accuracy: 0.8365\n",
            "Epoch 2/25\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.5107 - accuracy: 0.8292 - val_loss: 0.4027 - val_accuracy: 0.8716\n",
            "Epoch 3/25\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.4591 - accuracy: 0.8514 - val_loss: 0.3729 - val_accuracy: 0.8813\n",
            "Epoch 4/25\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.4335 - accuracy: 0.8628 - val_loss: 0.3562 - val_accuracy: 0.8870\n",
            "Epoch 5/25\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.4125 - accuracy: 0.8690 - val_loss: 0.3490 - val_accuracy: 0.8912\n",
            "Epoch 6/25\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.4038 - accuracy: 0.8746 - val_loss: 0.3528 - val_accuracy: 0.8907\n",
            "Epoch 7/25\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.3909 - accuracy: 0.8805 - val_loss: 0.3396 - val_accuracy: 0.8973\n",
            "Epoch 8/25\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.3822 - accuracy: 0.8835 - val_loss: 0.3315 - val_accuracy: 0.8988\n",
            "Epoch 9/25\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.3815 - accuracy: 0.8856 - val_loss: 0.3215 - val_accuracy: 0.9043\n",
            "Epoch 10/25\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.3722 - accuracy: 0.8870 - val_loss: 0.3431 - val_accuracy: 0.8965\n",
            "Epoch 11/25\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.3686 - accuracy: 0.8904 - val_loss: 0.3211 - val_accuracy: 0.9061\n",
            "Epoch 12/25\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.3633 - accuracy: 0.8916 - val_loss: 0.3278 - val_accuracy: 0.9008\n",
            "Epoch 13/25\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.3605 - accuracy: 0.8947 - val_loss: 0.3376 - val_accuracy: 0.9000\n",
            "Epoch 14/25\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.3597 - accuracy: 0.8938 - val_loss: 0.3228 - val_accuracy: 0.9072\n",
            "Epoch 15/25\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.3575 - accuracy: 0.8963 - val_loss: 0.3101 - val_accuracy: 0.9118\n",
            "Epoch 16/25\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.3522 - accuracy: 0.8977 - val_loss: 0.3181 - val_accuracy: 0.9103\n",
            "Epoch 17/25\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.3507 - accuracy: 0.8979 - val_loss: 0.3211 - val_accuracy: 0.9068\n",
            "Epoch 18/25\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.3474 - accuracy: 0.8991 - val_loss: 0.3099 - val_accuracy: 0.9119\n",
            "Epoch 19/25\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.3469 - accuracy: 0.8997 - val_loss: 0.3148 - val_accuracy: 0.9104\n",
            "Epoch 20/25\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.3481 - accuracy: 0.8991 - val_loss: 0.3169 - val_accuracy: 0.9080\n",
            "Epoch 21/25\n",
            "750/750 [==============================] - 4s 6ms/step - loss: 0.3441 - accuracy: 0.9011 - val_loss: 0.3109 - val_accuracy: 0.9146\n",
            "Epoch 22/25\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.3405 - accuracy: 0.9017 - val_loss: 0.3121 - val_accuracy: 0.9121\n",
            "Epoch 23/25\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.3391 - accuracy: 0.9049 - val_loss: 0.3111 - val_accuracy: 0.9128\n",
            "Epoch 24/25\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.3388 - accuracy: 0.9031 - val_loss: 0.3211 - val_accuracy: 0.9078\n",
            "Epoch 25/25\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.3376 - accuracy: 0.9055 - val_loss: 0.3103 - val_accuracy: 0.9144\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.3243 - accuracy: 0.9081\n",
            "Improved CNN Test accuracy: 0.9081000089645386\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is what we have change -\n",
        "\n",
        "**Increased epochs from 20 to 25**\n",
        "\n",
        "Improved CNN Test accuracy: **0.9081000089645386**\n",
        "\n",
        "\n",
        "\n",
        "Increasing the number of training epochs from 20 to 25 and observing an improvement in test accuracy suggests that the model was able to learn more from the data with the additional training iterations.\n",
        "\n",
        "1. **More Learning Opportunities:** Additional epochs provide the model extra rounds to learn from the data, enhancing its ability to capture underlying patterns.\n",
        "\n",
        "2. **Closer Convergence:** With more epochs, the optimization process has extended time to fine-tune the model's weights, potentially achieving a more optimal fit to the training data.\n",
        "\n",
        "3. **Diminishing Returns:** While extra epochs can initially improve model performance, there comes a point where further training doesn't yield significant benefits and could lead to overfitting.\n",
        "\n",
        "The improvement in test accuracy indicates that the model was indeed capable of further learning and that the additional epochs contributed positively to its ability to generalize from the training data to unseen test data.\n"
      ],
      "metadata": {
        "id": "63k_z0XLkyhG"
      }
    }
  ]
}